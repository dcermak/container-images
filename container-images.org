# -*- org-confirm-babel-evaluate: nil; -*-
#+AUTHOR: Dan ƒåerm√°k
#+DATE: August 28, 2025
#+EMAIL: dcermak@suse.com
#+TITLE: Demystifying Containers and Container Images

#+REVEAL_ROOT: ./node_modules/reveal.js/
#+REVEAL_THEME: simple
#+REVEAL_PLUGINS: (highlight notes history)
#+OPTIONS: toc:nil
#+REVEAL_DEFAULT_FRAG_STYLE: appear
#+REVEAL_INIT_OPTIONS: transition: 'none', hash: true
#+OPTIONS: num:nil toc:nil center:nil reveal_title_slide:nil
#+REVEAL_EXTRA_CSS: ./node_modules/@fortawesome/fontawesome-free/css/all.min.css
#+REVEAL_EXTRA_CSS: ./custom-style.css
#+REVEAL_HIGHLIGHT_CSS: ./node_modules/reveal.js/plugin/highlight/zenburn.css

#+REVEAL_TITLE_SLIDE: <h2 class="title">%t</h2>
#+REVEAL_TITLE_SLIDE: <p class="subtitle" style="color: Gray;">%s</p>
#+REVEAL_TITLE_SLIDE: <p class="author">%a</p>
#+REVEAL_TITLE_SLIDE: <div style="float:left"><a href="https://events.linuxfoundation.org/open-source-summit-europe/" target="_blank"><img src="./media/OSS-EU-logo-bg.svg" height="100px"/></a></div>
#+REVEAL_TITLE_SLIDE: <div style="float:right;font-size:35px;"><p xmlns:dct="http://purl.org/dc/terms/" xmlns:cc="http://creativecommons.org/ns#"><a href="https://creativecommons.org/licenses/by/4.0" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">
#+REVEAL_TITLE_SLIDE: CC BY 4.0 <i class="fab fa-creative-commons"></i> <i class="fab fa-creative-commons-by"></i></a></p></div>

* ~who -u~

Dan ƒåerm√°k

@@html: <div style="float:center">@@
@@html: <table class="who-table">@@
@@html: <tr><td><i class="fab fa-suse"></i></td><td> Software Developer @SUSE</td></tr>@@
@@html: <tr><td><i class="fab fa-fedora"></i></td><td> i3 SIG, Package maintainer</td></tr>@@
@@html: <tr><td><i class="far fa-heart"></i></td><td> Developer Tools, Testing and Documentation, Home Automation</td></tr>@@
@@html: <tr></tr>@@
@@html: <tr></tr>@@
@@html: <tr><td><i class="fa-solid fa-globe"></i></td><td> <a href="https://dancermak.name/">https://dancermak.name</a></td></tr>@@
@@html: <tr><td><i class="fab fa-github"></i></td><td> <a href="https://github.com/dcermak/">dcermak</a> </td></tr>@@
@@html: <tr><td><i class="fab fa-mastodon"></i></td><td> <a href="https://mastodon.social/@Defolos">@Defolos@mastodon.social</a></td></tr>@@
@@html: <tr><td><i class="fab fa-bluesky"></i></td><td> <a href="https://bsky.app/profile/defolos.bsky.social">@defolos.bsky.social</a></td></tr>@@
@@html: </table>@@
@@html: </div>@@


* Agenda

- [[Software Delivery][Containers from Scratch]]
- [[Introducing: Docker][Docker & Podman]]
- [[Container Orchestration][Container Orchestration]]
- [[Should I use containers?][Should I use containers?]]


* Software Delivery
#+begin_notes
- dependencies suck, dependency handling sucks, don't wanna handle 20 different library versions
- bundling solves /some/ problems
- VMs too heavy, too much software (bootloader, kernel), often not even necessary
\rightarrow build something lightweight
#+end_notes

#+ATTR_REVEAL: :frag (appear)
- dependencies suck \rightarrow bundle everything
- Can't we just ship everything?
- yes, but VMs suck as well ü´†


** Let's build a container: =chroot=

#+begin_notes
- build everything locally or in a pre-installed VM
- tar the whole thing
- clean up everything (logs, temporary build deps, etc)
#+end_notes

#+ATTR_REVEAL: :frag (appear)
1. Create a chroot: ~rsync -avz --exclude=/sysroot/ / /sysroot/~
2. build your app
3. clean =/sysroot= of everything unneeded
4. ~tar -czvf my-app.tar.gz /sysroot~


** Let's run the container

#+ATTR_REVEAL: :frag (appear)
1. ~tar -xzvf my-app.tar.gz -C /path/to/app~
2. ~chroot /path/to/app /usr/local/bin/my-app~


** Is that it?

#+ATTR_REVEAL: :frag (appear)
- inconvenient build process
- no upgrade path
- data sharing?
- no security


** Linux Namespaces
#+begin_notes
- introduced in 2002 (kernel 2.4), more added in 2006
- container support finished in 2013: user namespace with kernel 3.8
- user: separate user ids of namespace & host, map uids between host & namespace
  \Rightarrow uid 0 in namespace is user who created namespace
  (\rightarrow see also =/etc/subuid=)
- mnt: mount namespace, isolated mounts
- pid: Process ID isolation, process that "created" the namespace gets PID 1 and
  all other processes become its children (also of sub-namespaces)
- net: each net inerface in one namespace
- ipc: restrict SysV style IPC
- uts - unix time sharing: set hostname & domainname
- cgroup (added in 4.6): hide cgroup path, i.e. process only sees relative
  cgroup path of the namespace and no others
- time (added in 5.6): set different system time

- useful tool: lsns
- namespaces can be nested & inherit
#+end_notes

#+ATTR_REVEAL: :frag (appear)
- kernel level resource isolation

#+ATTR_REVEAL: :frag (appear)
available namespaces:

#+ATTR_REVEAL: :frag (appear)
- user
- mnt
- pid
- net
- ipc
- uts
- cgroup
- time

#+REVEAL: split

Try it:
#+ATTR_REVEAL: :frag (appear) :code_attribs data-line-numbers='1-3|4-5|6-9'
#+begin_src shell
$ unshare --user --map-root-user \
      --pid --fork --mount-proc \
      /bin/bash
# whoami
root
# ps -a
    PID TTY          TIME CMD
      1 pts/8    00:00:00 bash
    104 pts/8    00:00:00 ps
#+end_src


** cgroups

#+begin_notes
- started in 2006, merged in 2008 (2.6.24)
- redesigned to v2 in 2016 (4.5)

- resource limits like I/O, FS caches, CPU quota, open files
- process priorization
- measure whole group resource usage & freeze/restart it

- nowadays used by userspace memory killers
- modern DEs put each process into a cgroup \rightarrow for oom-killers
#+end_notes

#+ATTR_REVEAL: :frag (appear)
- apply resource limits to processes
- measure resource usage

#+ATTR_REVEAL: :frag (appear) :code_attribs data-line-numbers='1|2|3-4'
#+begin_src shell
# cgcreate -g memory:memlimit
# cgset -r memory.max=1K memlimit
# cgexec -g memory:memlimit ls -al
Killed
#+end_src


** Are we there yet?

#+ATTR_REVEAL: :frag (appear)
- üëçÔ∏è great process isolation
- üëéÔ∏è standardized build process
- üëéÔ∏è distribution mechanism


* Introducing: Docker

#+begin_notes
- solve the redistribution & build process
#+end_notes

#+ATTR_REVEAL: :frag (appear)
@@html:<img src="./media/Docker_(container_engine)_logo.svg"/>@@

#+ATTR_REVEAL: :frag (appear)
1. ~docker build~
2. Docker registry


** Docker Registry

#+ATTR_REVEAL: :frag (appear)
@@html:<img src="./media/registry.svg"/>@@

#+ATTR_REVEAL: :frag (appear) :code_attribs data-line-numbers='1|2|3'
#+begin_src bash
docker pull registry.opensuse.org/opensuse/leap
docker pull registry.opensuse.org/opensuse/leap:15.6
docker pull registry.opensuse.org/opensuse/leap:15.5@sha256:a5ecb8286a6a1b695acb17e63f2702be29f2a72615ec10cfb4e427e2ebc9e8ad
#+end_src

#+begin_notes
- central image storage, initially there was only [[https://hub.docker.com][Docker Hub]] (nowadays many registries exist)
- provides images via HTTP API
- images identified via =repository:tag@digest=
- repository: name of the image
- tag: something like a version, but really a free form field
  only special value is =:latest=, pulled by default
  you can have multiple images with the same tag üòí
- digest: sha256 or sha512 hash of the image manifest

Digests:
some background: OCI registries return to =GET
/v2/<repo>/manifests/<tag>= either a =distribution.manifest= or a
=distribution.manifest.list= (that's a list of =distribution.manifest=), the digest
of an image is the sha256sum/sha512sum of the =distribution.manifest=
#+end_notes


** Container Image Build

#+begin_notes
- fix the inconvenient build process
- build runs *in* a container!
- docker build standardized & simplified the image build process via the
  =Dockerfile=
- syntax is: =INSTRUCTION <value>=
- image build starts =FROM= an image specified using the same format as the
  registry
- each instruction creates a layer, changes put on top, build process relies
  heavily on caching
- container image is "just a bunch of tar balls"
#+end_notes

#+ATTR_REVEAL: :frag (appear)
#+begin_src bash
docker build .
#+end_src

#+ATTR_REVEAL: :frag (appear) :code_attribs data-line-numbers='|1|3|4|6-9|11|12|'
#+begin_src Dockerfile
FROM registry.opensuse.org/opensuse/tumbleweed

COPY . /src/
WORKDIR /src/

RUN zypper -n in python3-pip; \
    pip install . ; \
    zypper -n rm --clean-deps gcc; zypper -n clean; \
    rm -rf {/target,}/var/log/{alternatives.log,lastlog,tallylog,zypper.log,zypp/history,YaST2}

EXPOSE 80
CMD ["/usr/bin/python", "-m", "my-app"]
#+end_src

** UnionFS

#+begin_notes
- final image constructed from individual layers
- file precedence: "highest directory" over "lowest"
- file removal: via whiteout files,
  in overlayFS: character special file (device 0, 0), create via =mknod $path c 0 0=
  oci tar archives prepend =.wh.=, empty file
- directory removal: whiteout file
  oci tar archives: =dir/.wh..wh..opq= empty file
  in overlayFS: character special file in upper dir (again created via =mknod=)

catches:
- you can never truly delete files
- editing a file creates a full copy (unionFS works on a file level)
- certain operations not atomic
- directory renames are very ugly (delete + full copy)

- try it locally with OverlayFS on Linux,
  lowerdir: read only layers
  upperdir: rw top dir (= container dir)
  workdir: used for internal purposes (CoW)
#+end_notes

#+ATTR_REVEAL: :frag (appear)
@@html:<img src="./media/overlays.svg"/>@@

#+ATTR_REVEAL: :frag (appear)
#+begin_src bash
mount -t overlay overlay \
      -o lowerdir=lower_3:lower_2:lower_1,\
         upperdir=upper,workdir=/work/ \
           merged
#+end_src


** Dockerfile

#+begin_notes
- =FROM= - specifies the base image for the current build stage
- =COPY= - copy files from the current build context (the directory passed as last
  CLI arg) or from other stage to current stage
  =ADD= used to fill this use case, but discouraged nowadays
- =ENV=: set environment variables, global for rest of build stage & final image
- =RUN=: execute arbitrary commands in the container image context, using the
  default shell. Beware of shell escapes when creating multiline strings, often
  resort to hacks like [[https://stackoverflow.com/a/33439625][ksh93 ANSI-C quoting]]
  supports also flags like mounting secrets or setting the network
- =VOLUME=: declares a directory as a volume, everything in it is temporary from
  this layer on, when launching the container a temporary volume is created
- =WORKDIR=: sets the cwd for all subsequent instructions & for entrypoint/cmd
- =EXPOSE=: defines network ports to be exposed, but only documentation. protocol
  can be specified, defaults to TCP if not supplied. Ports still have to be
  exposed via =-p $hostPort:$ctrPort= or all via =-P=
- =USER=: defines the user for entrypoint & cmd and subsequent =RUN= instructions,
  must exist in the image!
- =CMD=: default args for the entrypoint
- =ENTRYPOINT=: defines binary launched as PID 1

additional directives:
- =ARG= - set build arguments, can be passed via =--build-arg "USER=me"= CLI flag
- =LABEL=: add key-value metadata to the image, common ones:
  https://github.com/opencontainers/image-spec/blob/main/annotations.md
- =SHELL=: sets the shell, defaults to =["/bin/sh", "-c"]=
- =STOPSIGNAL=: which signal should be sent to PID 1 on =docker stop= (defaults to
  =SIGTERM=)

non-standard:
- =HEALTHCHECK=: command to check whether application in container is up
- =ONBUILD=: commands executed when using this image for building
#+end_notes

#+ATTR_REVEAL: :frag (appear) :code_attribs data-line-numbers='|1|2|3|4-7|8|9|10|11|12|13-14|'
#+begin_src Dockerfile
FROM registry.opensuse.org/opensuse/tumbleweed
COPY ./project/ /src/
ENV USER="geeko"
RUN zypper -n in openssh-clients; \
    ssh-keygen -t ed25519 -f /root/.ssh/id_ed25519 -N ""; \
    zypper -n rm --clean-deps openssh-clients; \
    zypper -n clean; rm -rf /var/log/lastlog;
VOLUME ["/src/data"]
WORKDIR /src/
EXPOSE 22
RUN useradd $USER
USER $USER
CMD ["echo hello"]
ENTRYPOINT ["/bin/bash", "-ce"]
#+end_src

# Doesn't fit anywhere‚Ä¶
# ** Launching a Container

# #+ATTR_REVEAL: :frag (appear)
# 1. Lookup image locally
# 2. (optionally) pull the image
# 3. write layers to disk & setup UnionFS
# 4. setup namespaces & cgroups
# 5. setup networking
# 6. launch entrypoint using =runc= / =crun= / =$runtime=


** Volumes

#+begin_notes
- data of a container exist in the (somewhat) temporary =upper= dir
  \Rightarrow app data not persisted, must be mounted from external
1. bind mount
2. container volume (mount data provided by container engine, implementation
   defined, but usually folder)

- beware of SELinux! \Rightarrow (podman) launches container process with =container_t=
  label, can only access files with =container_file_t= label (not present *anywhere*
  by default) \Rightarrow =:Z= & =:z= flags relabel volumes and add this flag,
  see: https://www.redhat.com/en/blog/user-namespaces-selinux-rootless-containers
#+end_notes

#+ATTR_REVEAL: :frag (appear)
@@html:<img src="./media/volumes.svg"/>@@

#+ATTR_REVEAL: :frag (appear)
#+begin_src bash
docker run -v /vol/:/var/db/ -v logs:/var/log $img
#+end_src

** Entrypoint

#+begin_notes
- entrypoint is launched as PID 1 in pid namespace by OCI runtime
  \Rightarrow everything in PID namespace becomes child process
  \Rightarrow must forward signals to children & reap them

This is why containers are not mini-VMs!

- entrypoint should *not* be a shell \Rightarrow use the exec form and not the free form to
  define the =ENTRYPOINT=, i.e.: ~ENTRYPOINT ["//bin/foo//", "arg"]~
- entrypoint gets passed =CMD= as args by default
- entrypoint should handle custom args, e.g. to launch a shell then
- exec the actual container process, not just launch it as a subprocess (messes
  up signal handling)
- sign that signal handling is messed up:
  =WARN[0010] StopSignal SIGTERM failed to stop container $FOO in 10 seconds, resorting to SIGKILL=

- preferably don't run a full init like systemd (hardly doable with docker)
- general scheme: support configuration via environment variables
#+end_notes

#+ATTR_REVEAL: :frag (appear)
@@html:<img src="./media/entrypoint.svg"/>@@


** Networking

#+begin_notes
- containers use bridge network by default:
  can reach outside, but not other way around
  ports need to be explicitly exposed (in bridge networking)
- docker uses libnetwork to configure networking
- CNI is container networking interface for rootfull networking, asigns IPs,
  setup network interfaces & routin, uses plugins
  CNI is only used by docker in k8s mode with containerd
- major networking modes:
  * bridge: NAT bridge to host net
  * host: use same network as host
  * none
  * overlay: connects multiple docker networks
  * macvlan: container gets its own network interface with unique MAC
  * ipvlan: container gets its own IP

More information: https://web.archive.org/web/20240215124249/https://labs.iximiuz.com/tutorials/container-networking-from-scratch
#+end_notes

#+ATTR_REVEAL: :frag (appear)
@@html:<img src="./media/networking.svg"/>@@


** Best Practices

#+begin_notes
- ensure that layers don't leave stuff you don't need around
- entrypoint should launch one primary binary not two or 15
- configuration via env vars is nice :-)
- entrypoint should either drop you into a shell or launch an app, depends on
  the type of app
- don't run a full init like systemd please
- volumes are your friend to ensure data survive a container "death"
#+end_notes

#+ATTR_REVEAL: :frag (appear)
#+begin_src Dockerfile
RUN zypper -n in python3-pip; \
    pip install . ; \
    zypper -n rm --clean-deps gcc; zypper -n clean; \
    rm -rf {/target,}/var/log/{alternatives.log,lastlog,tallylog,zypper.log,zypp/history,YaST2}
#+end_src

#+REVEAL: split

#+ATTR_REVEAL: :frag (appear) :code-attribs data-line-numbers='1-3|4'
#+begin_src shell
$ podman run -e POSTGRES_PORT=1234 \
             -e POSTGRES_USER=pg \
                 my-app
$ podman run my-app bash
#
#+end_src

#+ATTR_REVEAL: :frag (appear)
or:
#+ATTR_REVEAL: :frag (appear)
#+begin_src shell
$ podman run my-app
#
#+end_src

#+REVEAL: split

Volumes are your friend:
#+ATTR_REVEAL: :frag (appear)
#+begin_src Dockerfile
VOLUME ["/var/db/"]
# /var/db/ is now erased after each step!
#+end_src

#+ATTR_REVEAL: :frag (appear)
use the exec-form:
#+ATTR_REVEAL: :frag (appear)
#+begin_src Dockerfile
ENTRYPOINT ["/usr/bin/my-app", "-param", "value"]
#+end_src


** Podman

#+begin_notes
- docker uses split architecture: CLI run as user, daemon performs actual heavy
  lifting
- daemon runs as *root* by default! \Rightarrow everyone with access to the daemon is
  effectively root!!
- disagreements between RH & Docker caused fork/new project: podman & buildah
- podman inner workings: https://www.redhat.com/en/blog/behind-scenes-podman
#+end_notes

#+ATTR_REVEAL: :frag (appear)
Actually Docker

#+ATTR_REVEAL: :frag (appear)
@@html:<img src="./media/docker-daemon.svg"/>@@

#+REVEAL: split

Podman

#+ATTR_REVEAL: :frag (appear)
@@html:<img src="./media/podman.svg"/>@@

** Rootless Containers

#+begin_notes
- container runtime executes as non-root, container process launched as non-root
- requires "relatively recent" kernel for user namespaces & =newuidmap= /
  =newgidmap= to be setuid root
  \Rightarrow users in container are mapped to uids/gids from =/etc/subuid= & =/etc/subgid=
- container has only your privileges, not more!
- cannot expose ports <= 1024
- firewall needs to be manually punched through
- rootless networking runs in userspace, e.g. libslirp/slirp4netns or pasta
  due to kernel requiring root privileges to create network namespaces
#+end_notes

#+ATTR_REVEAL: :frag (appear)
- container runs as non-root or a sub-uid of your user
- rootless networking runs in userspace

** Security

#+begin_notes
- container can potentially do anything your user can
  \Rightarrow docker socket is effectively root access!
  *but* added complexity & setuid binaries!
- possible to break out of containers!
- SELinux often can prevent access to host
#+end_notes

#+ATTR_REVEAL: :frag (appear)
- container potentially as privileged as the user running it
- container breakout attacks exist
- SELinux is your friend


** When to use

#+begin_notes
- app should have single entrypoint
#+end_notes

#+ATTR_REVEAL: :frag (appear)
- Single binary
- Cloud Native Deployment
- Testing other Distributions
- Reproducible Dev/Test/Build Environment
- Base OS doesn't matter

** When not to use

#+ATTR_REVEAL: :frag (appear)
- Complex multi binary legacy code
- High-Performance Code
- Base OS matters


* Container Orchestration

#+begin_notes
- each container = one binary \Rightarrow multiple containers for full app
- need something to launch containers, monitor them & tear down
- preferably from config file \Rightarrow managed via git
#+end_notes

#+ATTR_REVEAL: :frag (appear)
@@html:<img src="./media/container-orchestration.svg"/>@@


** docker-compose

#+begin_notes
- tool to launch multiple containers, defined via YAML file
- first beta in Dec 2013, 1.0 Oct 2014
  v2 Sep 2021 (rewrite in Go, changed format)
- define your whole app in one file, supports every docker/container feature
- supports service dependencies!
- supports scaling but needs an ingress/load balancer then
#+end_notes

#+ATTR_REVEAL: :frag (appear)
#+begin_src yaml
services:
  app:
    build: .
    ports:
      - "8080:8080"
    volumes:
      - .:/src
    depends_on:
      db:
        condition: service_healthy
  db:
    image: registry.opensuse.org/opensuse/mariadb
    environment:
      - MARIADB_ALLOW_EMPTY_ROOT_PASSWORD=1
#+end_src

#+ATTR_REVEAL: :frag (appear)
#+begin_src bash
docker compose up
#+end_src

** Quadlet / =podman generate systemd=

#+begin_notes
- original podman would generate systemd units
- nowadays: quadlet - simplified systemd unit file like syntax
  uses systemd generator to create systemd units
- systemd manages lifecycle

- see =man podman-systemd.unit= or
  https://docs.podman.io/en/latest/markdown/podman-systemd.unit.5.html
#+end_notes

#+ATTR_REVEAL: :frag (appear)
#+begin_src ini
[Unit]
Description=TW container

[Container]
Image=registry.opensuse.org/opensuse/tumbleweed

# volume and network defined below in other configs
Volume=test.volume:/data
Network=test.network

Exec=sleep infinity

[Service]
Restart=always
TimeoutStartSec=900

[Install]
# Start by default on boot
WantedBy=multi-user.target default.target
#+end_src

** Kubernetes

#+begin_notes
- originally started as "Borg" at Google
- open sourced 2014, donated to CNCF
- declarative configuration via kubernetes yaml
- self healing & (auto) horizontal scaling
- for microservice architecture (i.e. each container single app)
- became quickly industry standard, kubernetes yaml nowadays supported by podman

architecture:
- Control Plane (master components):
  - API Server: Front-end for the Kubernetes control plane
  - etcd: Consistent and highly-available key-value store for all cluster data
  - Scheduler: Assigns workloads to nodes
  - Controller Manager: Runs controller processes
  - Cloud Controller Manager: Integrates with cloud provider APIs
- Node Components:
  - Kubelet: Ensures containers are running in a pod
  - Container Runtime: Software responsible for running containers (Docker,
    containerd, CRI-O)
  - Kube-proxy: Network proxy that maintains network rules on nodes

Key Concepts:
- Pods: Smallest deployable units, containing one or more containers
- Services: Abstraction that defines a logical set of pods and a policy to access them
- Deployments: Manage the deployment and scaling of a set of pods
- ConfigMaps/Secrets: Ways to inject configuration into applications
- Namespaces: Virtual clusters within a physical cluster
- Persistent Volumes: Storage abstraction that outlives pod lifecycle

Common Patterns:
- Sidecar: Helper containers that enhance the main container
- Ambassador: Proxy local connections to external services
- Adapter: Standardizes and normalizes output of the main container
- Init Containers: Run before app containers, setting up dependencies
- StatefulSets: For stateful applications requiring stable network identifiers and persistent storage
- DaemonSets: Ensure that all nodes run a copy of a specific pod
- Jobs/CronJobs: Run-to-completion and scheduled tasks

- Kubernetes yaml
#+end_notes

# https://en.wikipedia.org/wiki/Kubernetes#/media/File:Kubernetes.png
#+ATTR_REVEAL: :frag (appear)
@@html:<img src="./media/Kubernetes.png" height="500px"/>@@

#+REVEAL: split

#+ATTR_REVEAL: :frag (appear)
#+begin_src yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-application
  labels:
    app: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: web-container
        image: nginx:latest
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: "0.5"
            memory: "512Mi"
          requests:
            cpu: "0.2"
            memory: "256Mi"
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
#+end_src

* Should I use containers?

#+begin_notes
pro:
- infrastructure as code
- data & app separated
- easier to test & deploy

con:
- added complexity
- added overhead + space requirements
- not suitable for all workloads
- can be problematic in certain setups (rootless + ldap)
#+end_notes

#+ATTR_REVEAL: :frag (appear)
It depends


* Questions?

@@html:<img src="./media/qr.svg" height="300px"/>@@

@@html:<i class="fa-solid fa-person-chalkboard"></i>@@ [[https://dcermak.github.io/container-images/container-images.html][=dcermak.github.io/container-images=]]
